## nginx优化
### worker_processes
worker_processes用来设置Nginx服务的进程数。推荐是CPU内核数或者内核数的倍数，推荐使用CPU内核数，因为我的CPU为4核的，所以设置为4。

### worker_cpu_affinity
默认情况下，Nginx的多个进程有可能跑在某一个CPU或CPU的某一核上，导致Nginx进程使用硬件的资源不均，因此绑定Nginx进程到不同的CPU上是为了充分利用硬件的多CPU多核资源的目的。
worker_cpu_affinity用来为每个进程分配CPU的工作内核，参数有多个二进制值表示，每一组代表一个进程，每组中的每一位代表该进程使用CPU的情况，1代表使用，0代表不使用。所以我们使用worker_cpu_affinity 0001 0010 0100 1000;来让进程分别绑定不同的核上。

### worker_connections
设置一个进程理论允许的最大连接数，理论上越大越好，但不可以超过worker_rlimit_nofile的值。还有个问题，linux系统中有个指令open file resource limit，它设置了进程可以打开的文件句柄数量，可以用下面的指令查看你的linux系统中open file resource limit指令的值，cat /proc/sys/fs/file-max
可以将该指令设置为23900251
echo "2390251" > /proc/sys/fs/file-max; sysctl -p

### worker_rlimit_nofile
设置毎个进程的最大文件打开数。如果不设的话上限就是系统的ulimit –n的数字，一般为65535。


### use epoll
设置事件驱动模型使用epoll。事件驱动模型有select、poll、poll等。
-   select先创建事件的描述符集合，对于一个描述符，可以关注其上面的Read事件、Write事件以及Exception事件，所以要创建三类事件描述符集合，分别用来处理Read事件的描述符、Write事件的描述符、Exception事件的描述符，然后调用底层的select()函数，等待事件发生，轮询所有事件描述符集合的每一个事件描述符，检查是否有事件发生，有的话就处理。select效率低，主要是轮询效率低，而且还要分别轮询三个事件描述符的集合。
-   poll方法与select类似，都是先创建一个关注事件的描述符集合，再去等待这些事件发生，然后再轮询描述符集合，检查有无事件发生，如果有，就去处理。不同点是poll为Read事件、Write事件以及Exception事件只创建一个集合，在每个描述符对应的结构上分别设置Read事件、Write事件以及Exception事件。最后轮询的时候，可以同时检察权这三个事件是否发生。可以说，poll库是select库的优化实现。
-   epoll是Nginx支持的高性能事件驱动库之一。是公认的非常优秀的事件驱动模型。和poll库跟select库有很大的不同，最大区别在于效率。我们知道poll库跟select库都是创建一个待处理的事件列表，然后把这个列表发给内核，返回的时候，再去轮询检查这个列表，以判断事件是否发生。这样在描述符多的应用中，效率就显得比较低下了。一种比较好的方式是把列表的管理交由内核负责，一旦某种事件发生，内核就把发生事件的描述符列表通知给进程，这样就避免了轮询整个描述符列表。首先，epoll库通过相关调用同志内核创建一个有N个描述符的事件列表，然后给这些描述符设置所关注的事件，并把它添加到内核的事件列表中去。完成设置以后，epoll库就开始等待内核通知事件发生了，某一事件发生后，内核讲发生事件的描述符列表上报给epoll库，得到事件列表的epoll库，就可以进行事件处理了。epoll库在linux平台是高效的，它支持一个进程打开大数目的事件描述符，上限是系统可以打开文件的最大数目；同时，epoll库的IO效率不随描述符数量的增加而线性下降，因为它只会对内核上报的活跃的描述符进行操作。

### accept_mutex
大致意思是当某一时刻只有一个网络连接到来时，多个睡眠进程会被同时叫醒，但只有一个进程可获得连接，如果每次唤醒的进程数目太多，会影响一部分系统性能。在Nginx服务器的多进程下，就可能出现这个问题，为了解决这个问题，Nginx配置了包含这样一条指令accept_mutex，当其设置为开启的时候，将会对多个Nginx进程接受连接进行序列化，防止多个进程对连接的争抢。当服务器连接数不多时，开启这个参数会让负载有一定程度的降低。但是当服务器的吞吐量很大时，为了效率，请关闭这个参数；并且关闭这个参数的时候也可以让请求在多个worker间的分配更均衡。所以我们设置accept_mutex off;

### sendfile
使用开启或关闭是否使用sendfile()传输文件，普通应用应该设为on，下载等IO重负荷的应用应该设为off，因为大文件不适合放到buffer中。
传统文件传输中（read／write方式）在实现上3其实是比较复杂的，需要经过多次上下文切换，当需要对一个文件传输时，传统方式是：
-   调用read函数，文件数据被copy到内核缓冲区
-   read函数返回，数据从内核缓冲区copy到用户缓冲区
-   write函数调用，将文件数据从用户缓冲区copy到内核与socket相关的缓冲区
-   数据从socket缓冲区copy到相关协议引擎
从上面可以看出来，传统readwrite进行网络文件传输的方式，在过程中经历了四次copy操作。

硬盘－>内核buffer->用户buffer->socket相关缓冲区->协议引擎
而sendfile系统调用则提供了一种减少多次copy，提高文件传输性能的方法。流程如下：
-   sendfile系统效用，文件数据被copy至内核缓冲区
-   记录数据文职和长度相关的数据保存到socket相关缓存区
-   实际数据由DMA模块直接发送到协议引擎

### tcp_nopush
sendfile为on时这里也应该设为on，数据包会累积一下再一起传输，可以提高一些传输效率。

### tcp_nodelay
小的数据包不等待直接传输。默认为on。
看上去是和tcp_nopush相反的功能，但是两边都为on时nginx也可以平衡这两个功能的使用。

### keepalive_timeout
HTTP连接的持续时间。设的太长会使无用的线程变的太多。这个根据自己服务器访问数量、处理速度以及网络状况方面考虑。

### send_timeout
设置Nginx服务器响应客户端的超时时间，这个超时时间只针对两个客户端和服务器建立连接后，某次活动之间的时间，如果这个时间后，客户端没有任何活动，Nginx服务器将关闭连接，将其设置为10s，Nginx与客户端建立连接后，某次会话中服务器等待客户端响应超过10s，就会自动关闭。

### types_hash_max_size
types_hash_max_size影响散列表的冲突率。types_hash_max_size越大，就会消耗更多的内存，但散列key的冲突率会降低，检索速度就更快。types_hash_max_size越小，消耗的内存就越小，但散列key的冲突率可能上升。

### client_header_buffer_size
该指令用于设置Nginx服务器允许的客户端请求头部的缓冲区大小，默认为1KB，此指令的赋值可以根据系统分页大小来设置，分页大小可以用以下命令获取getconf PAGESIZE。

### client_max_body_size 8m
客户端上传的body的最大值。超过最大值就会发生413(Request Entity Too Large)错误。默认为1m，最好根据自己的情况改大一点。
